主从复制机制可以允许我们拓展节点来进行数据拷贝，可以根据业务场景进行读写分离、数据备份等功能，但是主节点Master出现异常时并不能实现自动主从复制节点切换、故障处理转移等操作，本篇要介绍的**哨兵机制**正是基于Redis的主从复制机制进行的节点监听管理机制，可以在出现上述问题时进行节点切换和故障转移，是一种Redis**高可用**方案的实现机制。

# 架构拓扑

![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa15d34ce4344fd9bd9a404eec1831e5~tplv-k3u1fbpfcp-zoom-1.image)
- **Master（主节点）** Redis的主服务数据库，负责接收业务数据的写入，一般为一个（这里不扩展分布式架构下sharding后的水平扩展下多Master，仅简单讨论Redis Sentinel一般架构拓扑）
- **Slave（从节点）** Redis的从服务数据库，复制Master节点数据，一般为多个
- **Sentinel Node** Sentinel哨兵节点，负责监听Master、Slave等业务数据节点情况，一般为多个形成哨兵集群节点，这也是哨兵机制自身高可用的一种体现

组成 | 角色定位 | 作用 | 数量
----|-----|:----|----
Master| 业务数据主节点 | 接收客户端请求，可读可写 | 1
Slave | 业务数据从节点 | 复制Master数据，灾备，可写（读写分离）| >=1
Sentinel Node | 哨兵节点 | 监听Master、Slave业务数据节点，在故障发生时进行故障转移处理 | >=1

# 运行机制
<code>Redis Sentinel</code>主要工作任务就是时刻监听所有Redis节点的状态，一旦发生异常根据预设值机制进行故障处理使Redis可以达到高可用。核心实现机制是，通过**三个定时监控任务**完成对各个节点发现和监控。

定时任务|触发间隔|功能
-----|------|:-----
定时info |10s | Sentinel节点获取最新Redis节点信息和拓扑关系
定时publish/subscribe| 2s |Sentinel节点通过订阅master频道进行彼此信息通互
定时ping| 1s | Sentinel节点检查与所有Redis节点、其他Sentinel节点网络
![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/95b8dea28fda493ab4b1b526342de35f~tplv-k3u1fbpfcp-zoom-1.image)
- **[Worker-1]** 每隔10秒，每个Sentinel节点会向<code>master</code>和<code>slave</code>发送<code>info</code>命令获取 最新的拓扑结构。
  **该定时任务的作用是：** 当故障发生或有新节点加入时，可以定时获取和更新当前Redis节点的拓扑关系

> 在<code>master</code>节点执行<code>info replication</code>可查看主从复制信息如下：
Replication role:master connected_slaves:2
slave0:ip=127.0.0.1,port=6380,state=online,offset=4917,lag=1
slave1:ip=127.0.0.1,port=6381,state=online,offset=4917,lag=1`

![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/32a29feba3494b73889c4639a6e5814c~tplv-k3u1fbpfcp-zoom-1.image)
- **[Worker-2]** 每隔2秒，每个Sentinel节点会向Redis数据节点的<code>\__sentinel__：hello</code>频道上<code>发布（publish）</code>该Sentinel节点对于主节点的判断以及当前Sentinel节点的信息，同时每个Sentinel节点也会<code>订阅（subcribe）</code>该频道，来了解其他Sentinel节点以及它们对<code>master</code>的判断
  **该定时任务的作用是：** 所有的<code>sentinel节点</code>通过发布/订阅主节点的<code>\__sentinel__：hello</code>进行节点间信息通互，为后面**客观下线**以及**领导者选举**提供依据

![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/182023beabb548b989d0fc73077a8cc1~tplv-k3u1fbpfcp-zoom-1.image)

- **[Worker-3]** 每隔1秒，每个Sentinel节点会向<code>master</code>、<code>slave</code>、<code>其他sentinel节点</code>发送一条<code>ping</code>命令做一次**心跳检测**来确认这些节点当前是否可达
# 故障转移
![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e7678b67e6094d58b6ce99d43f306a3d~tplv-k3u1fbpfcp-zoom-1.image)
- **[step-1]** 当<code>sentinel node</code>节点监听到<code>master</code>节点出现故障，<code>slave</code>从节点无法对<code>master</code>进行数据复制
  ![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/24cf9079cd814b7e920ec596972b9167~tplv-k3u1fbpfcp-zoom-1.image)
- **[step-2]** <code>sentinel node</code>发现<code>master</code>节点异常，会在<code>sentinel集群节点</code>中内部进行投票选举出<code>leader</code>来进行<code>master</code>、<code>slave</code>业务数据节点故障进行转移处理，并通知<code>client</code>客户端
> **超时故障判断：**
> 通过<code>down-after-milliseconds</code>参数进行配置，当超过该时间无响应则判断为节点故障<br>
> **集群投票机制：**
> 由于<code>sentinel node</code>是以集群形式存在的，当<code>sentinel node</code>监听到<code>master</code>节点异常时，会询问其他<code>sentinel node</code>进行所有节点集群投票确认决定下一步是否进行，这样能很好的减少单节点对故障的误判

![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/616dd7d3548d415fb042465835e37cbc~tplv-k3u1fbpfcp-zoom-1.image)
- **[step-3]** 当新的<code>master</code>产生后，<code>slave</code>节点会复制新的<code>master</code>，但是还会继续监听旧的<code>master</code>节点
  ![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a7e5cda9e1a54196a443633fe0817267~tplv-k3u1fbpfcp-zoom-1.image)
- **[step-4]** 当旧的<code>master</code>节点故障恢复后，由于<code>sentinel集群</code>一直监听，会重新将其纳入集群管理中，将其变为新的<code>master</code>节点的从节点，此时恢复后的故障节点变为<code>slave</code>，开始复制新的<code>master</code>节点，实现节点故障后的重复利用

以上为<code>Redis Sentinel</code>架构下故障转移流程，总结以上流程的时序图交互如下：
![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c5ddec8cb4e04add9adb3dd9155cf405~tplv-k3u1fbpfcp-zoom-1.image)
# 集群选举
## Sentinel节点选举
由于<code>sentinel</code>是以集群形式存在来保证高可用，因此在故障处理时，需要先选举一个<code>sentinel节点</code>作为**Leader**进行操作，每一个<code>sentinel节点</code>都可以成为**Leader**。
![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/99c351b298844b9792e43ba4a97c65f2~tplv-k3u1fbpfcp-zoom-1.image)
**选举过程：**
- 当一个<code>sentinel节点</code>确认redis集群的主节点**下线**后
- 请求其他<code>sentinel节点</code>要求将自己选举为**Leader**。被请求的<code>sentinel节点</code>如果没有同意过其他<code>sentinel节点</code>的选举请求，则同意该请求，即**选举票数+1**，否则不同意。
- 当一个<code>sentinel节点</code>获得的选举票数达到**Leader**最低票数(<code>sentinel节点数/2+1的最大值</code>)，则该<code>sentinel节点</code>选举为**Leader**；否则重新进行选举。
> <code>Sentinel集群</code>采用的是<code>Raft算法</code>进行选举，感兴趣可以继续探究该算法内部实现机制。

**主观下线&客观下线：**
![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dfe4275ee4eb43858a9098c129efc441~tplv-k3u1fbpfcp-zoom-1.image)
-  **主观下线**
   Sentinel集群的每一个Sentinel节点会定时对redis集群的所有节点发心跳包检测节点是否正常。如果一个节点在<code>down-after-milliseconds</code>时间内没有回复Sentinel节点的心跳包，则该redis节点被该<code>sentinel节点</code>**主观下线**，所谓**主观下线**是单独一个节点判断，有可能此时该节点与<code>master</code>通信异常，而非<code>master</code>与全部节点交互异常，因此需要多个<code>sentinel节点</code>共同确认。<br>
- **客观下线**
  当节点被一个<code>sentinel节点</code>记为**主观下线**时，并不意味着该节点肯定故障了，还需要<code>sentinel集群</code>的其他<code>sentinel节点</code>共同判断为**主观下线**才行。

## Redis节点选举
当<code>sentinel集群</code>选举出<code>sentinel leader</code>后，由<code>sentinel leader</code>从<code>slave</code>中选择一个作为<code>master</code>。

**选举过程：**
- 过滤故障的节点
- 选择优先级<code>slave-priority</code>最大的<code>slave</code>作为<code>master</code>，如不存在则继续
- 选择<code>复制偏移量(offset)</code>（数据写入量的字节，记录写了多少数据。主服务器会把偏移量同步给从服务器，当主从的偏移量一致，则数据是完全同步）最大的<code>slave</code>作为<code>master</code>，如不存在则继续
- 选择<code>runid</code>（redis每次启动的时候生成随机的runid作为redis的标识）最小的<code>slave</code>作为<code>master</code>，这里是一个随机方案也是最终兜底方案
# 参考
《Redis设计与实现》

《Redis开发与运维》

[Sentinel节点选举](https://www.cnblogs.com/albert32/p/13393382.html )